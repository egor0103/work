{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Dataframes III\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Обзор источников данных\n",
    "+ Текстовые форматы txt, csv, json\n",
    "+ Parquet и ORC\n",
    "+ Elastic\n",
    "+ Cassandra\n",
    "+ PostgreSQL\n",
    "\n",
    "## Обзор источников данных\n",
    "Spark - это платформа для **обработки** распределенных данных. Она не отвечает за хранение данных и не завязана на какую-либо БД или формат хранения, что позволяет разработать коннектор для работы с любым источником. Часть распространенных источников доступна \"из коробки\", часть - в виде сторонних библиотек. \n",
    "\n",
    "На текущий момент Spark DF API позволяет работать (читать и писать) с большим набором источников:\n",
    "+ Текстовые файлы:\n",
    "  - [json](https://spark.apache.org/docs/latest/sql-data-sources-json.html)\n",
    "  - text\n",
    "  - [csv](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)\n",
    "+ Бинарные файлы:\n",
    "  - [orc](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)\n",
    "  - [parquet](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)\n",
    "  - [delta](https://docs.delta.io/latest/quick-start.html)\n",
    "+ Базы данных\n",
    "  - [elastic](https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html#spark-sql)\n",
    "  - [cassandra](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md)\n",
    "  - [jdbc](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\n",
    "  - [redis](https://github.com/RedisLabs/spark-redis/blob/master/doc/dataframe.md)\n",
    "  - [mongo](https://docs.mongodb.com/spark-connector/master/scala-api/)\n",
    "+ Стриминг системы\n",
    "  - [kafka](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "\n",
    "Для текстовых файлов поддерживаются различные кодеки сжатия (например `lzo`, `snappy`, `gzip`)\n",
    "\n",
    "### Добавление поддержки\n",
    "Чтобы добавить поддержку источника в проект, необходимо:\n",
    "+ найти нужный пакет на https://mvnrepository.com\n",
    " - выбрать актуальную версию для `Scala 2.11`\n",
    " - скачать jar или скопировать команду для нужной системы сборки \n",
    "+ добавить зависимость в `libraryDependencies` в файле `build.sbt`  \n",
    "```libraryDependencies += \"org.elasticsearch\" %% \"elasticsearch-spark-20\" % \"7.7.0\"```\n",
    "+ добавить зависимость в приложение одним из способов:\n",
    "  - добавить зависимость в **spark-submit**:  \n",
    "  ```spark-submit --packages org.elasticsearch:elasticsearch-spark-20_2.11:7.7.0```\n",
    "  - добавить jar файл в **spark-submit**:  \n",
    "  ```spark-submit --jars /path/to/elasticsearch-spark-20_2.11-7.7.0.jar```\n",
    "  - добавить зависимость в **spark-defaults.conf**:  \n",
    "  ```spark.jars.packages org.elasticsearch:elasticsearch-spark-20_2.11:7.7.0```\n",
    "  - добавить jar файл в **spark-defaults.conf**:  \n",
    "  ```spark.jars /path/to/elasticsearch-spark-20_2.11-7.7.0.jar```\n",
    "  - в коде через [`spark.sparkContext.addJar`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@addJar(path:String):Unit)\n",
    "  \n",
    "### Использование в коде\n",
    "Конфиги источника задаются одним из способов:\n",
    "- через **spark-submit**:  \n",
    "```spark-submit --conf spark.es.nodes=localhost:9200```\n",
    "- в **spark-defaults.conf**:  \n",
    "```spark.es.nodes localhost:9200```\n",
    "- в коде через **SparkSession**:\n",
    "  + ```spark.conf.set(\"spark.es.nodes\", \"localhost:9200\")```\n",
    "- в коде при чтении:  \n",
    "  + ```val df = spark.read.format(\"elastic\").option(\"es.nodes\", \"localhost:9200\")...```\n",
    "  + ```val df = spark.read.format(\"elastic\").options(Map(\"es.nodes\" -> \"localhost:9200\"))...```\n",
    "- в коде при записи:  \n",
    "  + ```df.write.format(\"elastic\").option(\"es.nodes\", \"localhost:9200\")...```\n",
    "  + ```df.write.format(\"elastic\").options(Map(\"es.nodes\" -> \"localhost:9200\"))...```\n",
    "  \n",
    "### Выводы:\n",
    "- Spark позволяет работать с болшим количеством источников\n",
    "- Поддержка источника всегда добавлеяется на уровне JVM (даже для pyspark) путем добавления в `java classpath` нужного класса\n",
    "- Добавить поддержку источника можно по-разному, однако в большинстве случаев следует избегать \"хардкода\"\n",
    "\n",
    "## Текстовые форматы\n",
    "\n",
    "Spark позволяет хранить данные в текстовом виде в форматах `text`, `json`, `csv`\n",
    "- `json` - JSON строки (не массив JSON документов, а именно раздельные строки, разделенные `\\n`)  \n",
    "- `csv` - плоские данные с разделителем  \n",
    "- `text` просто текстовые строки, вычитываются как DF с единственной колонкой `value: String`  \n",
    "\n",
    "### Преимущества:\n",
    "- простота интеграции\n",
    "- поддержка партиционирования и сжатия\n",
    "\n",
    "### Недостатки:\n",
    "- отсутствие оптимизаций \n",
    "- низкая скорость чтения сжатых данных\n",
    "- слабая типизация\n",
    "\n",
    "Прочитаем датасет [Airport Codes](https://datahub.io/core/airport-codes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 t3nq  wheel  6117301 Feb  7 18:03 /tmp/datasets/airport-codes.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "println(\"ls -al /tmp/datasets/airport-codes.csv\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "// val airports = \n",
    "//     spark.read\n",
    "//         .format(\"csv\")\n",
    "//         .option(\"path\", \"/tmp/datasets/airport-codes.csv\")\n",
    "//         .options(csvOptions).load()\n",
    "\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем его в формате `csv`. Запись данных происходит в директорию, внутри которой будут файлы с данными. Это свойство является общим для всех файловых форматов в Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "// airports.write.mode(\"overwrite\").csv(\"/tmp/datasets/airports-2.csv\")\n",
    "airports.write.format(\"csv\").mode(\"overwrite\").option(\"path\", \"/tmp/datasets/airports-2.csv\").save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12360\n",
      "drwxr-xr-x  8 t3nq  wheel      256 Feb  7 19:34 .\n",
      "drwxr-xr-x  5 t3nq  wheel      160 Feb  7 19:34 ..\n",
      "-rw-r--r--  1 t3nq  wheel        8 Feb  7 19:34 ._SUCCESS.crc\n",
      "-rw-r--r--  1 t3nq  wheel    33540 Feb  7 19:34 .part-00000-1774eeb3-dc7b-4ddf-aab8-fa4a1e84ed23-c000.csv.crc\n",
      "-rw-r--r--  1 t3nq  wheel    15440 Feb  7 19:34 .part-00001-1774eeb3-dc7b-4ddf-aab8-fa4a1e84ed23-c000.csv.crc\n",
      "-rw-r--r--  1 t3nq  wheel        0 Feb  7 19:34 _SUCCESS\n",
      "-rw-r--r--  1 t3nq  wheel  4291982 Feb  7 19:34 part-00000-1774eeb3-dc7b-4ddf-aab8-fa4a1e84ed23-c000.csv\n",
      "-rw-r--r--  1 t3nq  wheel  1974943 Feb  7 19:34 part-00001-1774eeb3-dc7b-4ddf-aab8-fa4a1e84ed23-c000.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -al /tmp/datasets/airports-2.csv\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы попытаемся прочитать его с помощью `spark.read`, используя старый код, получим ошибку - в качестве схемы Spark взял одну из строк, содержащую данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "Path does not exist: file:/tmp/datasets/airports-2.csv;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Path does not exist: file:/tmp/datasets/airports-2.csv;",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)",
      "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)",
      "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)",
      "  at scala.collection.immutable.List.flatMap(List.scala:355)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:468)",
      "  ... 42 elided"
     ]
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airports-2.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поищем шапку в сырых данных - ее там не будет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.text(\"/tmp/datasets/airports-2.csv\").filter('value contains \"elevation_ft\").count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если прочитать с `header=false`, названия колонок будут автоматически сгенерированы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n",
      "-RECORD 0----------------------------------\n",
      " _c0  | 00A                                \n",
      " _c1  | heliport                           \n",
      " _c2  | Total Rf Heliport                  \n",
      " _c3  | 11                                 \n",
      " _c4  | NA                                 \n",
      " _c5  | US                                 \n",
      " _c6  | US-PA                              \n",
      " _c7  | Bensalem                           \n",
      " _c8  | 00A                                \n",
      " _c9  | null                               \n",
      " _c10 | 00A                                \n",
      " _c11 | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> false, inferSchema -> true)\n",
       "airports = [_c0: string, _c1: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string, _c1: string ... 10 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"false\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airports-2.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имея шапку в виде строки, мы можем создать схему самостоятельно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates\n",
      ": string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates\n",
      " | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "firstLine = \n",
       "schema = \n",
       "csvOptions = Map(header -> false, inf...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"ident,type,name,elevation_ft,continent,iso_country,iso_region,municipality,gps_code,iata_code,local_code,coordinates\n",
       "\"\n",
       "StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,StringType,true), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates\n",
       ",StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(header -> false, inf..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val firstLine = \"head -n 1 /tmp/datasets/airport-codes.csv\".!!\n",
    "val schema = StructType(firstLine.split(\",\", -1).map(x => StructField(x, StringType)))\n",
    "val csvOptions = Map(\"header\" -> \"false\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.schema(schema).options(csvOptions).csv(\"/tmp/datasets/airports-2.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним данные в `csv` с включенной компрессией `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.repartition(1).write.mode(\"overwrite\").option(\"codec\", \"gzip\").csv(\"/tmp/datasets/airports-3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 31144\n",
      "drwxr-xr-x  8 t3nq  wheel   256B Feb  7 19:42 .\n",
      "drwxr-xr-x  6 t3nq  wheel   192B Feb  7 19:40 ..\n",
      "-rw-r--r--  1 t3nq  wheel     8B Feb  7 19:42 ._SUCCESS.crc\n",
      "-rw-r--r--  1 t3nq  wheel    16K Feb  7 19:40 .part-00000-82cd0df5-8903-4ef2-9a12-94574e8082f5-c000.csv.gz.crc\n",
      "-rw-r--r--  1 t3nq  wheel   104K Feb  7 19:42 .part-00000-98ddd864-6e94-46e6-9e78-3d30b4401e7b-c000.json.crc\n",
      "-rw-r--r--  1 t3nq  wheel     0B Feb  7 19:42 _SUCCESS\n",
      "-rw-r--r--  1 t3nq  wheel   2.1M Feb  7 19:40 part-00000-82cd0df5-8903-4ef2-9a12-94574e8082f5-c000.csv.gz\n",
      "-rw-r--r--  1 t3nq  wheel    13M Feb  7 19:42 part-00000-98ddd864-6e94-46e6-9e78-3d30b4401e7b-c000.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -alh /tmp/datasets/airports-3.csv\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [_c0: string, _c1: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\").option(\"path\", \"/tmp/datasets/airports-3.csv\").load\n",
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports\n",
    "    .repartition(1).write.mode(\"append\").option(\"codec\", \"gzip\").json(\"/tmp/datasets/airports-3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные стали занимать меньше места, но у этого решения есть существенный минус - при чтении каждый сжатый файл превращается ровно в 1 партицию в DF. При работе с большими датасетами это означает:\n",
    "- если файлов мало и они большие, то воркерам может не хватить памяти для их чтения (тк один сжатый файл нельзя разбить на несколько партиций\n",
    "- если файлов много и они маленькие - мы получаем увеличенный расход памяти в heap HDFS NameNode (память расходуется пропорционально количеству файлов на HDFS из расчета 1 ГБ памяти на 1 000 000 файлов)\n",
    "\n",
    "Сохраним датасет в формате `json` с партиционирование по колонкам `iso_region` и `iso_country`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.write.mode(\"overwrite\").partitionBy(\"iso_region\", \"iso_country\").json(\"/tmp/datasets/airports-1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x  4 t3nq  wheel   128B Feb  7 19:44 .\n",
      "drwxr-xr-x  3 t3nq  wheel    96B Feb  7 19:44 ..\n",
      "-rw-r--r--  1 t3nq  wheel    12B Feb  7 19:44 .part-00000-2114957a-21b2-4db9-a4f0-fd1ca52c7c98.c000.json.crc\n",
      "-rw-r--r--  1 t3nq  wheel   144B Feb  7 19:44 part-00000-2114957a-21b2-4db9-a4f0-fd1ca52c7c98.c000.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -alh /tmp/datasets/airports-1.json/iso_region=AD-04/iso_country=AD\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mySchemaString = `continent` STRING,`coordinates` STRING,`elevation_ft` BIGINT,`gps_code` STRING,`iata_code` STRING,`ident` STRING,`local_code` STRING,`municipality` STRING,`name` STRING,`type` STRING,`iso_region` STRING,`iso_country` STRING\n",
       "mySchema = StructType(StructField(continent,StringType,true), StructField(coordinates,StringType,true), StructField(elevation_ft,LongType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(ident,StringType,true), StructField(local_code,StringType,true), StructField(municipality,StringType,true), StructField(name,StringType,true), StructField(type,StringType,true), StructField(iso_region,StringType,true), StructField(iso_c...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(continent,StringType,true), StructField(coordinates,StringType,true), StructField(elevation_ft,LongType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(ident,StringType,true), StructField(local_code,StringType,true), StructField(municipality,StringType,true), StructField(name,StringType,true), StructField(type,StringType,true), StructField(iso_region,StringType,true), StructField(iso_c..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val mySchemaString = \"\"\"`continent` STRING,`coordinates` STRING,`elevation_ft` BIGINT,`gps_code` STRING,`iata_code` STRING,`ident` STRING,`local_code` STRING,`municipality` STRING,`name` STRING,`type` STRING,`iso_region` STRING,`iso_country` STRING\"\"\"\n",
    "\n",
    "val mySchema = DataType.fromDDL(mySchemaString).asInstanceOf[StructType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan json [continent#875,coordinates#876,elevation_ft#877L,gps_code#878,iata_code#879,ident#880,local_code#881,municipality#882,name#883,type#884,iso_region#885,iso_country#886] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/tmp/datasets/airports-1.json], PartitionCount: 1, PartitionFilters: [isnotnull(iso_region#885), isnotnull(iso_country#886), (iso_region#885 = AD-04), (iso_country#88..., PushedFilters: [], ReadSchema: struct<continent:string,coordinates:string,elevation_ft:bigint,gps_code:string,iata_code:string,i...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [continent: string, coordinates: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.schema(mySchema).json(\"/tmp/datasets/airports-1.json\")\n",
    "df.filter('iso_region === \"AD-04\" and 'iso_country === \"AD\").explain\n",
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(\"/tmp/foo/bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"total 28032\n",
       "drwxr-xr-x  214 t3nq  wheel   6.7K Feb  7 19:53 .\n",
       "drwxr-xr-x    3 t3nq  wheel    96B Feb  7 19:53 ..\n",
       "-rw-r--r--    1 t3nq  wheel     8B Feb  7 19:53 ._SUCCESS.crc\n",
       "-rw-r--r--    1 t3nq  wheel    38K Feb  7 19:53 .part-00000-9e8e9bdd-c629-4d3f-98df-fa72e4948cee-c000.json.crc\n",
       "-rw-r--r--    1 t3nq  wheel    16K Feb  7 19:53 .part-00001-9e8e9bdd-c629-4d3f-98df-fa72e4948cee-c000.json.crc\n",
       "-rw-r--r--    1 t3nq  wheel   8.0K Feb  7 19:53 .part-00002-9e8e9bdd-c629-4d3f-98df-fa72e4948cee-c000.json.crc\n",
       "-rw-r--r--    1 t3nq  wheel   4.9K Feb  7 19:53 .part-00003-9e8e9bdd-c629-4d3f-98df-fa72e4948cee-c000.json.crc\n",
       "-rw-r--r--    1 t3nq  wheel   3.6K Feb  7 19:53 .part-00004-9e8e9bdd-c629-4d3f-98df-fa72e4948cee-c000.json.crc\n",
       "-rw-r--r--    1 t3nq  wheel   3.0K Feb  7 19:53 .p...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"ls -alh /tmp/foo/bar\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [schemaofjson({ \"foo\" : \"bar\" }): string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "struct<foo:string>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = spark.range(1).select(schema_of_json(lit(\"\"\"{ \"foo\" : \"bar\" }\"\"\")))\n",
    "df.as[String].collect.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`continent` STRING,`coordinates` STRING,`elevation_ft` BIGINT,`gps_code` STRING,`iata_code` STRING,`ident` STRING,`local_code` STRING,`municipality` STRING,`name` STRING,`type` STRING,`iso_region` STRING,`iso_country` STRING"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.toDDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой формат хранения позволит использовать `partition pruning` и быстро фильтровать данные по колонкам `iso_region` и `iso_country`\n",
    "\n",
    "Теперь сохраним датасет в формат `text`. Для этого нам необходимо подгтовить DF, в котором будет единственная колонка `value: String`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports\n",
    "    .select('ident.alias(\"value\"))\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"text\")\n",
    "    .save(\"/tmp/datasets/airports-1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"ls -alh /tmp/datasets/airports-1.txt\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файловые форматы не имеют автоматической валидации данных при записи, поэтому достаточно легко ошибиться и записать данные в другом формате. Такая запись пройдет без ошибок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .json(\"/tmp/datasets/airports-1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При попытке чтения данных с помощью `text` мы получим все данные, тк форма `json` сохраняет все в виде JSON строк. Однако, если прочитать данные с помощью `json`, часть данных будут помечены как невалидные и помещены в колонку `_corrupt_record`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airports = spark.read.json(\"/tmp/datasets/airports-1.txt\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим невалидные JSON строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Начиная со Spark 2.3 нельзя выбирать одну колонку _corrupt_record, поэтому мы добавим к выводу ident\n",
    "airports.na.drop(\"all\", Seq(\"_corrupt_record\")).select($\"_corrupt_record\", $\"ident\").show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Режимы записи\n",
    "Spark позволяет нам выбирать режим записи данных с помощью метода `mode()`. Данный метод принимает один из параметров:\n",
    "- `overwrite` - перезаписывает всю директорию целиком (или партицию, если используется партиционирование)\n",
    "- `append` - дописывает новые файлы к текущим\n",
    "- `ignore` - не выполняет запись (no op режим)\n",
    "- `error` или `errorifexists` - возвращает ошибку, если директория уже существует"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семплирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Форматы `csv` и `json` позволяют автоматически выводить схему из данных. При этом по-умолчанию Spark прочитает все данные и составит подходящую схему. Однако, если мы работаем с большим датасетом, это может занять продолжительное время. Решить это можно с помощью опции `samplingRatio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "Time taken: 263 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time { \n",
    "    val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\", \"samplingRatio\" -> \"0.1\")\n",
    "    val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "    airports.printSchema\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "Time taken: 265 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time { \n",
    "    val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\", \"samplingRatio\" -> \"1.0\")\n",
    "    val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "    airports.printSchema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- Spark позволяет работать с текстовыми файлами `json`, `csv`, `text`\n",
    "- При чтении и записи поддерживаются кодеки сжатия данных, это создает дополнительные накладные расходы\n",
    "- При записи данных в текстовые форматы Spark **не выполняет** валидацию схемы и формата\n",
    "- При включенном выведении схемы из источника чтение из текстовых форматов происходит дольше\n",
    "\n",
    "## Orc и Parquet\n",
    "В отличие от обычных текстовых форматов, ORC и Parquet изначально спроектированы под распределенные системы хранения и обработки. Они являются колоночными - в них есть колонки и схема, как в таблицах БД и бинарными - прочитать обычным текстовым редактором их не получится. Форматы имеют похожие показатели производительности и архитектуру, но Parquet используется чаще\n",
    "\n",
    "### Преимущества\n",
    "- наличие схемы данных\n",
    "- блочная компрессия \n",
    "- для каждого блока для каждой колонки вычисляется max и min, что позволяет ускорять чтение\n",
    "\n",
    "### Недостатки:\n",
    "- нельзя дописывать/менять данные в существующих файлах\n",
    "- необходимо делать compaction\n",
    "\n",
    "Подробнее о Parquet:  \n",
    "[Фёдор Лаврентьев, Moscow Spark #5: Как класть Parquet](https://youtu.be/VHsvr10b63c?t=512)\n",
    "\n",
    "По аналогии с текстовыми форматами, при записи, Spark создает директорию и пишет туда все непустые партиции.  Обратите внимание на последовательность форматов записи - `snappy.parquet` вместо, скажем, `json.gz`. При использовании компрессии сам parquet файл не помещается в сжатый контейнер. Вместо этого, компрессии подлежат блоки с данными. Это полностью снимает ограничение, из-за которого чтение сжатых текстовых файлов происходит в 1 поток в 1 партицию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.write.mode(\"overwrite\").parquet(\"/tmp/datasets/airports-1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8384\n",
      "drwxr-xr-x  8 t3nq  wheel   256B Feb  7 20:23 .\n",
      "drwxr-xr-x  8 t3nq  wheel   256B Feb  7 20:23 ..\n",
      "-rw-r--r--  1 t3nq  wheel     8B Feb  7 20:23 ._SUCCESS.crc\n",
      "-rw-r--r--  1 t3nq  wheel    17K Feb  7 20:23 .part-00000-6ff24119-ba57-4148-b862-894ccfe5cca0-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 t3nq  wheel   7.8K Feb  7 20:23 .part-00001-6ff24119-ba57-4148-b862-894ccfe5cca0-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 t3nq  wheel     0B Feb  7 20:23 _SUCCESS\n",
      "-rw-r--r--  1 t3nq  wheel   2.1M Feb  7 20:23 part-00000-6ff24119-ba57-4148-b862-894ccfe5cca0-c000.snappy.parquet\n",
      "-rw-r--r--  1 t3nq  wheel   994K Feb  7 20:23 part-00001-6ff24119-ba57-4148-b862-894ccfe5cca0-c000.snappy.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"ls -alh /tmp/datasets/airports-1.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/tmp/datasets/airports-1.parquet\").printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с ORC/Parquet, часто возникает вопрос эволюции схемы - изменения структуры данных относительно первоначальных файлов. Создадим два DF с разными схемами и запишем их в одну директорию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Apple\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Apple(size: Int, color: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "List(Apple(1, \"green\")).toDS.write.mode(\"overwrite\").parquet(\"/tmp/datasets/apples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class PriceApple\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class PriceApple(size: Int, color: String, price: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(PriceApple(1, \"green\", 2.0)).toDS.write.mode(\"append\").parquet(\"/tmp/datasets/apples.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что файлы имеют разную схему, Spark корректно читает файлы, используя обобщенную схему:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|size|color|price|\n",
      "+----+-----+-----+\n",
      "|   1|green|  2.0|\n",
      "|   1|green| null|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [size: int, color: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[size: int, color: string ... 1 more field]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.parquet(\"/tmp/datasets/apples.parquet\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, это работает только тогда, когда мы добавляем новые колонки к нашей схеме. Если мы запишем новый файл, изменив тип уже существующей колонки, мы получим ошибку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class AppleBase\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class AppleBase(size: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1 = [size: int, color: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(size,IntegerType,true), StructField(color,StringType,true))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.read.format(\"parquet\").option(\"path\", \"/tmp/datasets/apples.parquet\").load\n",
    "df1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val path = \"/tmp/datasets/apples.parquet\"\n",
    "val toWrite = List(PriceApple(1, \"green\", 2.0)).toDS\n",
    "\n",
    "val mySchema = spark.read.parquet(path)\n",
    "if (mySchema == toWrite.schema) { \n",
    "    toWrite.write.mode(\"append\").parquet(\"/tmp/datasets/apples.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(AppleBase(3.0)).toDS.write.mode(\"append\").parquet(\"/tmp/datasets/apples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Failed merging schema:\nroot\n |-- size: double (nullable = false)\n",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Failed merging schema:",
      "root",
      " |-- size: double (nullable = false)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$mergeSchemasInParallel$1.apply(ParquetFileFormat.scala:643)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$mergeSchemasInParallel$1.apply(ParquetFileFormat.scala:639)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:639)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)",
      "  at scala.Option.orElse(Option.scala:289)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)",
      "  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)",
      "  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:629)",
      "  ... 60 elided",
      "Caused by: org.apache.spark.SparkException: Failed to merge fields 'size' and 'size'. Failed to merge incompatible data types int and double",
      "  at org.apache.spark.sql.types.StructType$$anonfun$merge$1$$anonfun$apply$3.apply(StructType.scala:502)",
      "  at org.apache.spark.sql.types.StructType$$anonfun$merge$1$$anonfun$apply$3.apply(StructType.scala:495)",
      "  at scala.Option.map(Option.scala:146)",
      "  at org.apache.spark.sql.types.StructType$$anonfun$merge$1.apply(StructType.scala:495)",
      "  at org.apache.spark.sql.types.StructType$$anonfun$merge$1.apply(StructType.scala:492)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at org.apache.spark.sql.types.StructType$.merge(StructType.scala:492)",
      "  at org.apache.spark.sql.types.StructType.merge(StructType.scala:402)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$mergeSchemasInParallel$1.apply(ParquetFileFormat.scala:641)",
      "  ... 74 more"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"true\")\n",
    "val df = spark.read.parquet(\"/tmp/datasets/apples.parquet\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим все доступные опции для работы с Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>key</th><th>value</th><th>meaning</th></tr><tr><td>spark.sql.parquet.binaryAsString</td><td>false</td><td>Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</td></tr><tr><td>spark.sql.parquet.columnarReaderBatchSize</td><td>4096</td><td>The number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.</td></tr><tr><td>spark.sql.parquet.compression.codec</td><td>snappy</td><td>Sets the compression codec used when writing Parquet files. If either `compression` or `parquet.compression` is specified in the table-specific options/properties, the precedence would be `compression`, `parquet.compression`, `spark.sql.parquet.compression.codec`. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, zstd.</td></tr><tr><td>spark.sql.parquet.enableVectorizedReader</td><td>true</td><td>Enables vectorized parquet decoding.</td></tr><tr><td>spark.sql.parquet.filterPushdown</td><td>true</td><td>Enables Parquet filter push-down optimization when set to true.</td></tr><tr><td>spark.sql.parquet.int64AsTimestampMillis</td><td>false</td><td>(Deprecated since Spark 2.3, please set spark.sql.parquet.outputTimestampType.) When true, timestamp values will be stored as INT64 with TIMESTAMP_MILLIS as the extended type. In this mode, the microsecond portion of the timestamp value will betruncated.</td></tr><tr><td>spark.sql.parquet.int96AsTimestamp</td><td>true</td><td>Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</td></tr><tr><td>spark.sql.parquet.int96TimestampConversion</td><td>false</td><td>This controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive & Spark.</td></tr><tr><td>spark.sql.parquet.mergeSchema</td><td>false</td><td>When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</td></tr><tr><td>spark.sql.parquet.outputTimestampType</td><td>INT96</td><td>Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.</td></tr><tr><td>spark.sql.parquet.recordLevelFilter.enabled</td><td>false</td><td>If true, enables Parquet's native record-level filtering using the pushed down filters. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting 'spark.sql.parquet.enableVectorizedReader' to false.</td></tr><tr><td>spark.sql.parquet.respectSummaryFiles</td><td>false</td><td>When true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn't be enabled before knowing what it means exactly.</td></tr><tr><td>spark.sql.parquet.writeLegacyFormat</td><td>false</td><td>If true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet's fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.</td></tr></table>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 20\n",
    "spark.sql(\"SET -v\").filter('key contains \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Tools\n",
    "Для диагностики и решения проблем, связанных с parquet, можно использовать утилиту `parquet-tools`\n",
    "\n",
    "Она позволяет:\n",
    "- получить схему файла\n",
    "- вывести содержимое файла в консоль\n",
    "- объединить несколько файлов в один\n",
    "\n",
    "https://github.com/apache/parquet-mr/tree/master/parquet-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение скорости обработки запросов\n",
    "\n",
    "Подготовим датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 to 50 foreach { x =>\n",
    "    airports.repartition(1).write.mode(\"append\").parquet(\"/tmp/datasets/a1.parquet\")\n",
    "    airports.repartition(1).write.mode(\"append\").json(\"/tmp/datasets/a1.json\")\n",
    "    airports.repartition(1).write.mode(\"append\").orc(\"/tmp/datasets/a1.orc\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DatasetFormat\n",
       "datasets = List(DatasetFormat([ident: string, type: string ... 10 more fields],orc), DatasetFormat([ident: string, type: string ... 10 more fields],parquet), DatasetFormat([continent: string, coordinates: string ... 10 more fields],json))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(DatasetFormat([ident: string, type: string ... 10 more fields],orc), DatasetFormat([ident: string, type: string ... 10 more fields],parquet), DatasetFormat([continent: string, coordinates: string ... 10 more fields],json))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "case class DatasetFormat[T](ds: Dataset[T], format: String)\n",
    "\n",
    "val datasets = \n",
    "    DatasetFormat(spark.read.orc(\"/tmp/datasets/a1.orc\"), \"orc\") ::\n",
    "    DatasetFormat(spark.read.parquet(\"/tmp/datasets/a1.parquet\"), \"parquet\") ::\n",
    "    DatasetFormat(spark.read.json(\"/tmp/datasets/a1.json\"), \"json\") ::\n",
    "    Nil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним скорость работы фильтраци:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running orc\n",
      "20500\n",
      "Time taken: 713 ms\n",
      "Running parquet\n",
      "20500\n",
      "Time taken: 524 ms\n",
      "Running json\n",
      "20500\n",
      "Time taken: 2394 ms\n"
     ]
    }
   ],
   "source": [
    "datasets.foreach { x => \n",
    "    println(s\"Running ${x.format}\")\n",
    "    spark.time {\n",
    "        println(x.ds.filter($\"iso_country\" === \"RU\" and $\"elevation_ft\" > 300).count)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним скорость подсчета количества строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running orc\n",
      "2811300\n",
      "Time taken: 175 ms\n",
      "Running parquet\n",
      "2811300\n",
      "Time taken: 129 ms\n",
      "Running json\n",
      "2811300\n",
      "Time taken: 2010 ms\n"
     ]
    }
   ],
   "source": [
    "datasets.foreach { x => \n",
    "    println(s\"Running ${x.format}\")\n",
    "    spark.time {\n",
    "        println(x.ds.count)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Форматы `orc` и `parquet` позволяют эффективно работать со структурированными данными\n",
    "- Производительность `orc` и `parquet` на порядок выше обычных текстовых файлов\n",
    "- Данные форматы поддерживают сжатие на блочном уровне, что позволяет избегать проблем с многопоточным чтением\n",
    "- Форматы поддерживают добавление новых колонок в схему, но не изменение текущих\n",
    "\n",
    "## Elastic\n",
    "Документориентированная распределенная база данных.\n",
    "\n",
    "### Преимущества:\n",
    "- Удобный графический интерфейс Kibana\n",
    "- Полнотекстовый поиск по любым колонкам\n",
    "- Встроенная поддержка timeseries\n",
    "- Поддержка вложенных структур\n",
    "- Возможность записи данных с произвольной схемой\n",
    "- Возможность перезаписывать данные по ключу документа\n",
    "\n",
    "### Недостатки:\n",
    "- Ассиметричная архитектура\n",
    "- Скорость записи ограничена самой медленным узлом\n",
    "- Большие накладные расходы CPU на индексирование\n",
    "- Ротация шардов не всегда проходит гладко\n",
    "\n",
    "https://www.elastic.co\n",
    "\n",
    "### Запуск в docker\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-dev-mode  \n",
    "https://www.elastic.co/guide/en/kibana/current/docker.html#_running_kibana_on_docker_for_development  \n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20  \n",
    "https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html  \n",
    "\n",
    "В Elastic есть несколько основных сущностей:\n",
    "\n",
    "**Index** - представляет собой \"таблицу с данными\", если проводить аналогию с реляционными БД. Данные в elastic обычно хранятся в виде индексов, разбитых на сутки (foo-2020-05-30, foo-2020-05-29 и т. д.). У каждого документа в индексе есть ключ `_id` и может быть метка времени, по которой в Kibana строятся визуализации\n",
    "\n",
    "**Template** - шаблон с параметрами, с которыми создается новый индекс. Пример шаблона представлен ниже:\n",
    "```shell\n",
    "PUT _template/airports\n",
    "{\n",
    "  \"index_patterns\": [\"airports-*\"],\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 1\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"_doc\": {\n",
    "      \"dynamic\": true,\n",
    "      \"_source\": {\n",
    "        \"enabled\": true\n",
    "      },\n",
    "      \"properties\": {\n",
    "        \"ts\": {\n",
    "          \"type\": \"date\",\n",
    "          \"format\": \"strict_date_optional_time||epoch_millis\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "**Shard** - индексы в elastic делятся ~~почкованием~~ на шарды. Это позволяет хранить индекс на нескольких узлах кластера\n",
    "\n",
    "**Index Pattern** - шаблон, применяемый к индексу на уровне Kibana. Позволяет настраивать форматирование и подсветку полей\n",
    "\n",
    "Перед тем, как начать писать в elastic с помощью Spark, нам необходимо создать шаблон, иначе индекс будет создан с параметрами по умолчанию и построить красивый pie chart в Kibana у нас не получится. Это можно сделать с помощью Dev Tools в Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "esOptions = Map(es.nodes -> localhost:9200, es.batch.write.refresh -> false, es.nodes.wan.only -> true)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(es.nodes -> localhost:9200, es.batch.write.refresh -> false, es.nodes.wan.only -> true)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val esOptions = \n",
    "    Map(\n",
    "        \"es.nodes\" -> \"localhost:9200\", \n",
    "        \"es.batch.write.refresh\" -> \"false\",\n",
    "        \"es.nodes.wan.only\" -> \"true\"   \n",
    "    )\n",
    "\n",
    "airports\n",
    "    .withColumn(\"ts\", current_timestamp())\n",
    "    .withColumn(\"date\", current_date())\n",
    "    .withColumn(\"foo\", lit(\"foo\"))\n",
    "    .write.format(\"es\").options(esOptions).save(\"airports-{date}/_doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- date: long (nullable = true)\n",
      " |-- elevation_ft: long (nullable = true)\n",
      " |-- foo: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "-RECORD 0-------------------------------\n",
      " continent    | SA                      \n",
      " coordinates  | -73.868056, 9.942222    \n",
      " date         | 1644181200000           \n",
      " elevation_ft | 256                     \n",
      " foo          | foo                     \n",
      " gps_code     | null                    \n",
      " iata_code    | null                    \n",
      " ident        | SK-164                  \n",
      " iso_country  | CO                      \n",
      " iso_region   | CO-CES                  \n",
      " local_code   | BCO                     \n",
      " municipality | Bosconia                \n",
      " name         | Julia Carolina Airport  \n",
      " ts           | 2022-02-07 21:08:05.108 \n",
      " type         | small_airport           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "esDf = [continent: string, coordinates: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, coordinates: string ... 13 more fields]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val esDf = spark.read.format(\"es\").options(esOptions).load(\"airports-*\")\n",
    "esDf.printSchema\n",
    "esDf.show(1, 200, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество партций в DF совпадает с общим числом шардов индексов, которые мы указали в `load()`. Поскольку у нас 1 индекс и 1 шард (см. шаблон), данный DF имеет 1 партицию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esDf.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К применяемым фильтрам применяется оптимизация `filter pushdown`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(iso_region#5195) && Contains(iso_region#5195, RU))\n",
      "+- *(1) Scan ElasticsearchRelation(Map(es.nodes -> localhost:9200, es.nodes.wan.only -> true, es.resource -> airports-*, es.batch.write.refresh -> false),org.apache.spark.sql.SQLContext@10c75cde,None) [continent#5186,coordinates#5187,date#5188L,elevation_ft#5189L,foo#5190,gps_code#5191,iata_code#5192,ident#5193,iso_country#5194,iso_region#5195,local_code#5196,municipality#5197,name#5198,ts#5199,type#5200] PushedFilters: [IsNotNull(iso_region), StringContains(iso_region,RU)], ReadSchema: struct<continent:string,coordinates:string,date:bigint,elevation_ft:bigint,foo:string,gps_code:st...\n"
     ]
    }
   ],
   "source": [
    "esDf.filter('iso_region contains \"RU\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (('ts >= cast(2020-06-01 16:57:30.000 as timestamp)) && ('ts <= cast(2020-06-01 16:59:30.000 as timestamp)))\n",
      "+- Relation[continent#5186,coordinates#5187,date#5188L,elevation_ft#5189L,foo#5190,gps_code#5191,iata_code#5192,ident#5193,iso_country#5194,iso_region#5195,local_code#5196,municipality#5197,name#5198,ts#5199,type#5200] ElasticsearchRelation(Map(es.nodes -> localhost:9200, es.nodes.wan.only -> true, es.resource -> airports-*, es.batch.write.refresh -> false),org.apache.spark.sql.SQLContext@10c75cde,None)\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, coordinates: string, date: bigint, elevation_ft: bigint, foo: string, gps_code: string, iata_code: string, ident: string, iso_country: string, iso_region: string, local_code: string, municipality: string, name: string, ts: timestamp, type: string\n",
      "Filter ((ts#5199 >= cast(2020-06-01 16:57:30.000 as timestamp)) && (ts#5199 <= cast(2020-06-01 16:59:30.000 as timestamp)))\n",
      "+- Relation[continent#5186,coordinates#5187,date#5188L,elevation_ft#5189L,foo#5190,gps_code#5191,iata_code#5192,ident#5193,iso_country#5194,iso_region#5195,local_code#5196,municipality#5197,name#5198,ts#5199,type#5200] ElasticsearchRelation(Map(es.nodes -> localhost:9200, es.nodes.wan.only -> true, es.resource -> airports-*, es.batch.write.refresh -> false),org.apache.spark.sql.SQLContext@10c75cde,None)\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter ((isnotnull(ts#5199) && (ts#5199 >= 1591019850000000)) && (ts#5199 <= 1591019970000000))\n",
      "+- Relation[continent#5186,coordinates#5187,date#5188L,elevation_ft#5189L,foo#5190,gps_code#5191,iata_code#5192,ident#5193,iso_country#5194,iso_region#5195,local_code#5196,municipality#5197,name#5198,ts#5199,type#5200] ElasticsearchRelation(Map(es.nodes -> localhost:9200, es.nodes.wan.only -> true, es.resource -> airports-*, es.batch.write.refresh -> false),org.apache.spark.sql.SQLContext@10c75cde,None)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter ((isnotnull(ts#5199) && (ts#5199 >= 1591019850000000)) && (ts#5199 <= 1591019970000000))\n",
      "+- *(1) Scan ElasticsearchRelation(Map(es.nodes -> localhost:9200, es.nodes.wan.only -> true, es.resource -> airports-*, es.batch.write.refresh -> false),org.apache.spark.sql.SQLContext@10c75cde,None) [continent#5186,coordinates#5187,date#5188L,elevation_ft#5189L,foo#5190,gps_code#5191,iata_code#5192,ident#5193,iso_country#5194,iso_region#5195,local_code#5196,municipality#5197,name#5198,ts#5199,type#5200] PushedFilters: [IsNotNull(ts), GreaterThanOrEqual(ts,2020-06-01 16:57:30.0), LessThanOrEqual(ts,2020-06-01 16:59..., ReadSchema: struct<continent:string,coordinates:string,date:bigint,elevation_ft:bigint,foo:string,gps_code:st...\n"
     ]
    }
   ],
   "source": [
    "esDf\n",
    "    .filter(\n",
    "        'ts between (\n",
    "                    lit(\"2020-06-01 16:57:30.000\").cast(\"timestamp\"), \n",
    "                    lit(\"2020-06-01 16:59:30.000\").cast(\"timestamp\")\n",
    "        )\n",
    "    ).explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Elastic - удобное распределенное хранилище документов, не накладывающее строгих ограничений на схему документов\n",
    "- Elastic позволяет делать сложные запросы, включая полнотекстовые\n",
    "- При работе с elastic, Spark часто использует `filter pushdown`\n",
    "- Spark отлично подходит для того, чтобы писать в elastic. Однако чтение работает не очень быстро.\n",
    "\n",
    "## Cassandra\n",
    "Распределенная табличная база данных\n",
    "\n",
    "### Преимущества\n",
    "- Высокая доступность данных\n",
    "- Мжожно строить гео-кластера\n",
    "- Высокая скорость записи и чтения\n",
    "- Скорость ограничена самым быстрым узлом\n",
    "- Линейная масштабируемость\n",
    "- Возможность хранить БОЛЬШИЕ объемы данных\n",
    "- Возможность быстро получать строку по ключу на любом объеме данных\n",
    "\n",
    "### Недостатки\n",
    "- слабая согласованность (eventual)\n",
    "- Бедный SQL (в кассандре он называется CQL)\n",
    "- Отсутствие транзакций (не совсем)\n",
    "\n",
    "https://cassandra.apache.org\n",
    "\n",
    "Cassandra имеет симметричную архитектуру. Каждый узел отвечает за хранение данных, обработку запросов и состояние кластера. \n",
    "\n",
    "Расположение данных определяется значением хеш функции от Partition key.\n",
    "\n",
    "Высокая доступность данных обеспечивается за счет репликации.\n",
    "\n",
    "![Cassandra Architecture](https://cassandra.apache.org/doc/latest/_images/ring.svg)\n",
    "Источник: https://cassandra.apache.org/doc/latest/architecture/dynamo.html#dataset-partitioning-consistent-hashing\n",
    "\n",
    "### Запуск в docker\n",
    "\n",
    "Запуск инстанса:\n",
    "```shell\n",
    "docker run --rm --name cass -p 9042:9042 -e CASSANDRA_BROADCAST_ADDRESS=127.0.0.1 cassandra:latest\n",
    "```\n",
    "\n",
    "Подключение к cassandra:\n",
    "```shell\n",
    "docker run -it --rm cassandra:latest cqlsh host.docker.internal\n",
    "```\n",
    "\n",
    "В Cassandra есть:\n",
    "- `keyspace` - аналог database - логическое объединение таблиц. На уровне keyspace устанавливается фактор репликации\n",
    "- `table` - таблицы, как в обычной БД\n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector  \n",
    "https://github.com/datastax/spark-cassandra-connector#documentation\n",
    "\n",
    "Для того, чтобы записать данные в cassandra, нам необходимо создать keyspace, используя утилиту `cqlsh`:\n",
    "```shell\n",
    "CREATE KEYSPACE IF NOT EXISTS airports \n",
    "WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3};\n",
    "```\n",
    "\n",
    "Теперь нужно создать таблицу со схемой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS airports.codes (ident text PRIMARY KEY,type text,name text,elevation_ft int,continent text,iso_country text,iso_region text,municipality text,gps_code text,iata_code text,local_code text,coordinates text);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "typesMap = Map(string -> text, int -> int)\n",
       "primaryKey = ident\n",
       "ddlColumns = ident text PRIMARY KEY,type text,name text,elevation_ft int,continent text,iso_country text,iso_region text,municipality text,gps_code text,iata_code text,local_code text,coordinates text\n",
       "ddlQuery = CREATE TABLE IF NOT EXISTS airports.codes (ident text PRIMARY KEY,type text,name text,elevation_ft int,continent text,iso_country text,iso_region text,municipality text,gps_code text,iata_code text,local_code text,coordinates text);\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CREATE TABLE IF NOT EXISTS airports.codes (ident text PRIMARY KEY,type text,name text,elevation_ft int,continent text,iso_country text,iso_region text,municipality text,gps_code text,iata_code text,local_code text,coordinates text);"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val typesMap = Map(\"string\" -> \"text\", \"int\" -> \"int\")\n",
    "\n",
    "val primaryKey = \"ident\"\n",
    "\n",
    "val ddlColumns = airports.schema.fields.map { x =>\n",
    "    if(x.name == primaryKey) {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)} PRIMARY KEY\"\n",
    "    }\n",
    "    else {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)}\"\n",
    "    }\n",
    "}.mkString(\",\")\n",
    "\n",
    "val ddlQuery = s\"CREATE TABLE IF NOT EXISTS airports.codes ($ddlColumns);\"\n",
    "\n",
    "println(ddlQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроим параметры подключения к БД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tableOpts = Map(table -> codes, keyspace -> airports)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(table -> codes, keyspace -> airports)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.cassandra._\n",
    "\n",
    "spark.conf.set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "spark.conf.set(\"spark.cassandra.output.consistency.level\", \"ANY\")\n",
    "spark.conf.set(\"spark.cassandra.input.consistency.level\", \"ONE\")\n",
    "\n",
    "val tableOpts = Map(\"table\" -> \"codes\",\"keyspace\" -> \"airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем записать датасет в БД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports\n",
    "    .write\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .mode(\"append\")\n",
    "    .options(tableOpts)\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------\n",
      " ident        | VE-0107                 \n",
      " continent    | SA                      \n",
      " coordinates  | -65.533333, 3.166667    \n",
      " elevation_ft | 375                     \n",
      " gps_code     | SVLE                    \n",
      " iata_code    | null                    \n",
      " iso_country  | VE                      \n",
      " iso_region   | VE-Z                    \n",
      " local_code   | null                    \n",
      " municipality | La Esmeralda            \n",
      " name         | Aeropuerto La Esmeralda \n",
      " type         | small_airport           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [ident: string, continent: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, continent: string ... 10 more fields]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "  .read\n",
    "  .format(\"org.apache.spark.sql.cassandra\")\n",
    "  .options(tableOpts)\n",
    "  .load()\n",
    "\n",
    "df.show(1, 200, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорость чтения ОЧЕНЬ сильно зависит от структуры таблицы и запроса. Если мы сделаем запрос по колонке `ident`, которая является ключом, то будет применена оптимизация `filter pushdown` и запрос отработает очень быстро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter isnotnull(ident#5302)\n",
      "+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@267e6964 [ident#5302,continent#5303,coordinates#5304,elevation_ft#5305,gps_code#5306,iata_code#5307,iso_country#5308,iso_region#5309,local_code#5310,municipality#5311,name#5312,type#5313] PushedFilters: [IsNotNull(ident), *EqualTo(ident,22WV)], ReadSchema: struct<ident:string,continent:string,coordinates:string,elevation_ft:int,gps_code:string,iata_cod...\n",
      "-RECORD 0--------------------------------------------\n",
      " ident        | 22WV                                 \n",
      " continent    | NA                                   \n",
      " coordinates  | -80.0291976928711, 39.34360122680664 \n",
      " elevation_ft | 1271                                 \n",
      " gps_code     | 22WV                                 \n",
      " iata_code    | null                                 \n",
      " iso_country  | US                                   \n",
      " iso_region   | US-WV                                \n",
      " local_code   | 22WV                                 \n",
      " municipality | Grafton                              \n",
      " name         | Grafton City Hospital Heliport       \n",
      " type         | heliport                             \n",
      "\n",
      "Time taken: 242 ms\n"
     ]
    }
   ],
   "source": [
    "df.filter('ident === \"22WV\").explain()\n",
    "\n",
    "spark.time { \n",
    "    df.filter('ident === \"22WV\").show(1, 200, true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если же мы сделаем запрос по другой колонке, то он не будет так же эффективен, хотя `filter pushdown` тоже отработает. Это происходит из-за того, что зная ключ, Cassandra знает, на каком хосте и где находятся данные. Когда мы фильтруем по колонке, которая не является ключом, БД приходится искать эти данные на всем кластере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(iso_region#5309) && (iso_region#5309 = RU))\n",
      "+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@267e6964 [ident#5302,continent#5303,coordinates#5304,elevation_ft#5305,gps_code#5306,iata_code#5307,iso_country#5308,iso_region#5309,local_code#5310,municipality#5311,name#5312,type#5313] PushedFilters: [IsNotNull(iso_region), EqualTo(iso_region,RU)], ReadSchema: struct<ident:string,continent:string,coordinates:string,elevation_ft:int,gps_code:string,iata_cod...\n",
      "(0 rows)\n",
      "\n",
      "Time taken: 2120 ms\n"
     ]
    }
   ],
   "source": [
    "df.filter('iso_region === \"RU\").explain()\n",
    "\n",
    "spark.time {\n",
    "    df.filter('iso_region === \"RU\").show(1, 200, true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если сделать запрос более сложным, то `filter pushdown` не отработает и Spark прочитает всю таблицу целиком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (lower('iso_region) = ru)\n",
      "+- Relation[ident#5302,continent#5303,coordinates#5304,elevation_ft#5305,gps_code#5306,iata_code#5307,iso_country#5308,iso_region#5309,local_code#5310,municipality#5311,name#5312,type#5313] org.apache.spark.sql.cassandra.CassandraSourceRelation@267e6964\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ident: string, continent: string, coordinates: string, elevation_ft: int, gps_code: string, iata_code: string, iso_country: string, iso_region: string, local_code: string, municipality: string, name: string, type: string\n",
      "Filter (lower(iso_region#5309) = ru)\n",
      "+- Relation[ident#5302,continent#5303,coordinates#5304,elevation_ft#5305,gps_code#5306,iata_code#5307,iso_country#5308,iso_region#5309,local_code#5310,municipality#5311,name#5312,type#5313] org.apache.spark.sql.cassandra.CassandraSourceRelation@267e6964\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (lower(iso_region#5309) = ru)\n",
      "+- Relation[ident#5302,continent#5303,coordinates#5304,elevation_ft#5305,gps_code#5306,iata_code#5307,iso_country#5308,iso_region#5309,local_code#5310,municipality#5311,name#5312,type#5313] org.apache.spark.sql.cassandra.CassandraSourceRelation@267e6964\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (lower(iso_region#5309) = ru)\n",
      "+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@267e6964 [ident#5302,continent#5303,coordinates#5304,elevation_ft#5305,gps_code#5306,iata_code#5307,iso_country#5308,iso_region#5309,local_code#5310,municipality#5311,name#5312,type#5313] PushedFilters: [], ReadSchema: struct<ident:string,continent:string,coordinates:string,elevation_ft:int,gps_code:string,iata_cod...\n",
      "(0 rows)\n",
      "\n",
      "Time taken: 1118 ms\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "df.filter(lower('iso_region) === \"ru\").explain(true)\n",
    "\n",
    "spark.time {\n",
    "    df.filter(lower('iso_region) === \"ru\").show(1, 200, true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо понимать, что структура данной таблицы не отражает реального паттерна работой с данной базой - редко ключом является только одна колонка. В реальности ключ является составным и состоит из нескольких колонок, что позволяет делать более сложные запросы, которые будут обрабатываться также быстро, как первый запрос с фильтрацией по колонке \"ident\"\n",
    "\n",
    "### Выводы\n",
    "- cassandra - одна из немногих БД, которая способна эффективно хранить большие объемы данных\n",
    "- в cassandra структура таблицы формируется, исходя из запросов, которые будут выполняться, а не наоборот\n",
    "- в данной БД данные обычно хранятся в денормализованном виде (если утрировать - то по таблице на каждый запрос)\n",
    "- Spark отлично подходит для чтения и записи данных в cassandra, но ее придется настроить под данный профиль нагрузки\n",
    "\n",
    "## PostgreSQL\n",
    "Классическая РБД\n",
    "\n",
    "### Преимущества:\n",
    "- это PostgreSQL\n",
    "\n",
    "### Недостатки\n",
    "- нет их\n",
    "\n",
    "### Запуск в docker\n",
    "```shell\n",
    "docker run --rm -p 5432:5432 --name test_postgre -e POSTGRES_PASSWORD=12345 postgres:latest\n",
    "```\n",
    "\n",
    "Подключение с помощью psql:\n",
    "```shell\n",
    "docker run -it --rm postgres psql -h host.docker.internal -U postgres\n",
    "```\n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/org.postgresql/postgresql\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n",
    "\n",
    "Для работы с БД создадим database:\n",
    "```shell\n",
    "CREATE DATABASE airports\n",
    "```\n",
    "Подготовим DDL для создания таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS codes (ident VARCHAR (100) PRIMARY KEY,type VARCHAR (100),name VARCHAR (100),elevation_ft INTEGER,continent VARCHAR (100),iso_country VARCHAR (100),iso_region VARCHAR (100),municipality VARCHAR (100),gps_code VARCHAR (100),iata_code VARCHAR (100),local_code VARCHAR (100),coordinates VARCHAR (100));\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "typesMap = Map(string -> VARCHAR (100), int -> INTEGER)\n",
       "primaryKey = ident\n",
       "ddlColumns = ident VARCHAR (100) PRIMARY KEY,type VARCHAR (100),name VARCHAR (100),elevation_ft INTEGER,continent VARCHAR (100),iso_country VARCHAR (100),iso_region VARCHAR (100),municipality VARCHAR (100),gps_code VARCHAR (100),iata_code VARCHAR (100),local_code VARCHAR (100),coordinates VARCHAR (100)\n",
       "ddlQuery = CREATE TABLE IF NOT EXISTS codes (ident VARCHAR (100) PRIMARY KEY,type VARCHAR (100),name VARCHAR (100),elevation_ft INTEGER,continent VARCHAR (100),iso_country VARCHAR (100),iso_region VARCHAR (100),municipality VARCHAR (100),gps_code VARCHAR (100),iata_code VARCHAR (100),local_code VARCHAR (100),coordinates VARCHAR (100));\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "jdbcUrl: ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CREATE TABLE IF NOT EXISTS codes (ident VARCHAR (100) PRIMARY KEY,type VARCHAR (100),name VARCHAR (100),elevation_ft INTEGER,continent VARCHAR (100),iso_country VARCHAR (100),iso_region VARCHAR (100),municipality VARCHAR (100),gps_code VARCHAR (100),iata_code VARCHAR (100),local_code VARCHAR (100),coordinates VARCHAR (100));"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val typesMap = Map(\"string\" -> \"VARCHAR (100)\", \"int\" -> \"INTEGER\")\n",
    "\n",
    "val primaryKey = \"ident\"\n",
    "\n",
    "val ddlColumns = airports.schema.fields.map { x =>\n",
    "    if(x.name == primaryKey) {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)} PRIMARY KEY\"\n",
    "    }\n",
    "    else {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)}\"\n",
    "    }\n",
    "}.mkString(\",\")\n",
    "\n",
    "val ddlQuery = s\"CREATE TABLE IF NOT EXISTS codes ($ddlColumns);\"\n",
    "\n",
    "val jdbcUrl = \"jdbc:postgresql://localhost/airports?user=postgres&password=12345\"\n",
    "\n",
    "println(ddlQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем данные в БД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.write.format(\"jdbc\").option(\"url\", jdbcUrl).option(\"dbtable\", \"codes\").mode(\"append\").save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0----------------------------------------------\n",
      " ident        | OK94                                   \n",
      " type         | small_airport                          \n",
      " name         | Sand Ridge Airpark Inc Airport         \n",
      " elevation_ft | 675                                    \n",
      " continent    | NA                                     \n",
      " iso_country  | US                                     \n",
      " iso_region   | US-OK                                  \n",
      " municipality | Collinsville                           \n",
      " gps_code     | OK94                                   \n",
      " iata_code    | null                                   \n",
      " local_code   | OK94                                   \n",
      " coordinates  | -95.80329895019531, 36.354801177978516 \n",
      "-RECORD 1----------------------------------------------\n",
      " ident        | OK95                                   \n",
      " type         | small_airport                          \n",
      " name         | Disney Airport                         \n",
      " elevation_ft | 955                                    \n",
      " continent    | NA                                     \n",
      " iso_country  | US                                     \n",
      " iso_region   | US-OK                                  \n",
      " municipality | Disney                                 \n",
      " gps_code     | OK95                                   \n",
      " iata_code    | null                                   \n",
      " local_code   | OK95                                   \n",
      " coordinates  | -94.95829772949219, 36.48899841308594  \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", jdbcUrl)\n",
    "    .option(\"dbtable\", \"codes\")\n",
    "    .load()\n",
    "\n",
    "df.printSchema\n",
    "df.show(2, 200, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании параметром по умолчанию мы получаем всего 1 партицию в DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправить это можно, используя параметры `partitionColumn`, `lowerBound`, `upperBound`, `numPartitions`. Для этого нам понадобится добавиь новую колонку в нашу таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS codes_x (ident VARCHAR (100) PRIMARY KEY,type VARCHAR (100),name VARCHAR (100),elevation_ft INTEGER,continent VARCHAR (100),iso_country VARCHAR (100),iso_region VARCHAR (100),municipality VARCHAR (100),gps_code VARCHAR (100),iata_code VARCHAR (100),local_code VARCHAR (100),coordinates VARCHAR (100),id INTEGER);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ddlColumns = ident VARCHAR (100) PRIMARY KEY,type VARCHAR (100),name VARCHAR (100),elevation_ft INTEGER,continent VARCHAR (100),iso_country VARCHAR (100),iso_region VARCHAR (100),municipality VARCHAR (100),gps_code VARCHAR (100),iata_code VARCHAR (100),local_code VARCHAR (100),coordinates VARCHAR (100),id INTEGER\n",
       "ddlQuery = CREATE TABLE IF NOT EXISTS codes_x (ident VARCHAR (100) PRIMARY KEY,type VARCHAR (100),name VARCHAR (100),elevation_ft INTEGER,continent VARCHAR (100),iso_country VARCHAR (100),iso_region VARCHAR (100),municipality VARCHAR (100),gps_code VARCHAR (100),iata_code VARCHAR (100),local_code VARCHAR (100),coordinates VARCHAR (100),id INTEGER);\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CREATE TABLE IF NOT EXISTS codes_x (ident VARCHAR (100) PRIMARY KEY,type VARCHAR (100),name VARCHAR (100),elevation_ft INTEGER,continent VARCHAR (100),iso_country VARCHAR (100),iso_region VARCHAR (100),municipality VARCHAR (100),gps_code VARCHAR (100),iata_code VARCHAR (100),local_code VARCHAR (100),coordinates VARCHAR (100),id INTEGER);"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ddlColumns = airports.schema.fields.map { x =>\n",
    "    if(x.name == primaryKey) {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)} PRIMARY KEY\"\n",
    "    }\n",
    "    else {\n",
    "        s\"${x.name} ${typesMap(x.dataType.simpleString)}\"\n",
    "    }\n",
    "} :+ \"id INTEGER\" mkString(\",\")\n",
    "\n",
    "val ddlQuery = s\"CREATE TABLE IF NOT EXISTS codes_x ($ddlColumns);\"\n",
    "\n",
    "println(ddlQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перезапишем данные в новую таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "airports\n",
    "    .withColumn(\"id\", round(rand() * 10000).cast(\"int\"))\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", jdbcUrl)\n",
    "    .option(\"dbtable\", \"codes_x\")\n",
    "    .mode(\"append\").save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем таблицу, установив дополнительные параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "-RECORD 0-------------------------------------\n",
      " ident        | OK99                          \n",
      " type         | closed                        \n",
      " name         | Unity Health Center Heliport  \n",
      " elevation_ft | 1073                          \n",
      " continent    | NA                            \n",
      " iso_country  | US                            \n",
      " iso_region   | US-OK                         \n",
      " municipality | Shawnee                       \n",
      " gps_code     | null                          \n",
      " iata_code    | null                          \n",
      " local_code   | null                          \n",
      " coordinates  | -96.93665, 35.362864          \n",
      " id           | 209                           \n",
      "-RECORD 1-------------------------------------\n",
      " ident        | OKV                           \n",
      " type         | small_airport                 \n",
      " name         | Okao Airport                  \n",
      " elevation_ft | 450                           \n",
      " continent    | OC                            \n",
      " iso_country  | PG                            \n",
      " iso_region   | PG-WPD                        \n",
      " municipality | Okao                          \n",
      " gps_code     | AYOF                          \n",
      " iata_code    | OKV                           \n",
      " local_code   | OKO                           \n",
      " coordinates  | 141.032777778, -5.55666666667 \n",
      " id           | 205                           \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [ident: string, type: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 11 more fields]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", jdbcUrl)\n",
    "    .option(\"dbtable\", \"codes_x\")\n",
    "    .option(\"partitionColumn\", \"id\")\n",
    "    .option(\"lowerBound\", \"0\")\n",
    "    .option(\"upperBound\", \"100000\")\n",
    "    .option(\"numPartitions\", \"200\")\n",
    "    .load()\n",
    "\n",
    "df.printSchema\n",
    "df.show(2, 200, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, сколько партиций получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим распределение данных по партициям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|12                  |2864 |\n",
      "|1                   |2811 |\n",
      "|13                  |2867 |\n",
      "|6                   |2831 |\n",
      "|16                  |2753 |\n",
      "|3                   |2870 |\n",
      "|20                  |4    |\n",
      "|5                   |2890 |\n",
      "|19                  |2819 |\n",
      "|15                  |2740 |\n",
      "|9                   |2817 |\n",
      "|17                  |2920 |\n",
      "|4                   |2846 |\n",
      "|8                   |2769 |\n",
      "|7                   |2819 |\n",
      "|10                  |2707 |\n",
      "|11                  |2782 |\n",
      "|14                  |2653 |\n",
      "|2                   |2908 |\n",
      "|0                   |2813 |\n",
      "|18                  |2743 |\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "df.groupBy(spark_partition_id()).count().show(200, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM table WHERE 1 = 0 LIMIT 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM table WHERE id > 0 AND id < 10;\n",
    "SELECT * FROM table WHERE id > 10 AND id < 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет работать с PostgreSQL через JDBC коннектор\n",
    "- При использовании `jdbc` настройка партиционирования задается вручную"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
